{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import h5py\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a span checker that analyzes how many subpages there are.\n",
    "# This might change on a daily basis, which is why we need this function.\n",
    "\n",
    "\n",
    "def span_checker(spans):\n",
    "    max_num = 0\n",
    "    for span in spans:\n",
    "        try:\n",
    "            int_span = int(span.text)\n",
    "            if int_span > max_num:\n",
    "                max_num = int_span\n",
    "        except ValueError:\n",
    "            pass\n",
    "    print(f\"The page contains {max_num} subpages\")\n",
    "    return max_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url, select_statement, site_type, title):\n",
    "    max_num = 0\n",
    "\n",
    "    # first, get the first site to access the navigation of it.\n",
    "    # hence Yahoo is only on page site, the function will stop here if yahoo is scrapped.\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    if site_type == \"yahoo\":\n",
    "        button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"btn.secondary.reject-all\"))\n",
    "        )\n",
    "        button.click()\n",
    "        WebDriverWait(driver, 10).until(EC.title_contains(title))\n",
    "        html_content = driver.page_source\n",
    "        soup = BeautifulSoup(html_content, \"lxml\")\n",
    "        driver.quit()\n",
    "        return soup\n",
    "\n",
    "    if site_type == \"boerse\":\n",
    "        html_content = driver.page_source\n",
    "        soup = BeautifulSoup(html_content, \"lxml\")\n",
    "        driver.quit()\n",
    "        return soup\n",
    "\n",
    "    html_content = driver.page_source\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "    # second check how many pages there are\n",
    "    spans = soup.select(select_statement)\n",
    "    max_num = span_checker(spans)\n",
    "\n",
    "    # third, access all the sites\n",
    "    soup_list = []\n",
    "    print(\"the following pages have been scrapped:\")\n",
    "    for i in range(1, max_num + 1):\n",
    "        if site_type == \"booking\":\n",
    "            url = url[:-1]\n",
    "            driver.get(url + str(1 + (i - 1) * 25))\n",
    "            print(url + str(1 + (i - 1) * 25))\n",
    "            html_content = driver.page_source\n",
    "            soup = BeautifulSoup(html_content, \"lxml\")\n",
    "            soup_list.append(soup)\n",
    "        elif site_type == \"immowelt\":\n",
    "            print(url + str(i))\n",
    "            driver.get(url + str(i))\n",
    "            html_content = driver.page_source\n",
    "            soup = BeautifulSoup(html_content, \"lxml\")\n",
    "            soup_list.append(soup)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # close the driver and\n",
    "    driver.quit()\n",
    "\n",
    "    # finally create the combined soup with all pages\n",
    "    global combined_soup\n",
    "    combined_soup = BeautifulSoup(\"\", \"lxml\")\n",
    "    for soup in soup_list:\n",
    "        combined_soup.append(soup)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the HDF5 Save Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime_str():\n",
    "    now = datetime.datetime.now()\n",
    "    now_str = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return now_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "# creating the HDF5 save file\n",
    "\n",
    "filename = \"scrapping_data.h5\"\n",
    "today_str = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "now_str = get_datetime_str()\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    print(\"file already exists\")\n",
    "\n",
    "else:\n",
    "    print(\"creating a new file\")\n",
    "    f = h5py.File(filename, \"w\")\n",
    "\n",
    "    # Create groups\n",
    "    f.create_group(\"immowelt\")\n",
    "    f.create_group(\"booking\")\n",
    "    f.create_group(\"boerse\")\n",
    "\n",
    "    # Create subgroup for immowelt group\n",
    "    f[\"immowelt\"].create_group(today_str)\n",
    "    f[\"immowelt\"][today_str].attrs[\"saved_datetime\"] = now_str\n",
    "\n",
    "    # Create subgroup for booking group\n",
    "    f[\"booking\"].create_group(today_str)\n",
    "    f[\"booking\"][today_str].attrs[\"saved_datetime\"] = now_str\n",
    "\n",
    "    # Create subgroup for boerse group\n",
    "    f[\"boerse\"].create_group(today_str)\n",
    "    f[\"boerse\"][today_str].attrs[\"saved_datetime\"] = now_str\n",
    "\n",
    "    # Close HDF5 file\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the save function\n",
    "def save_to_hdf5(filename, df, site, special_var):\n",
    "    now_str = get_datetime_str()\n",
    "    # saving immowelt data\n",
    "    path = f\"/{site}/{today_str}/df_{site}_{special_var}{today_str}/\"\n",
    "\n",
    "    f = h5py.File(filename, \"a\")\n",
    "    if today_str not in f[site]:\n",
    "        f[site].create_group(today_str)\n",
    "        f[site][today_str].attrs[\"saved_datetime\"] = now_str\n",
    "\n",
    "    else:\n",
    "        print(f\"entry for {today_str} already exists. Data will be appended\")\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    hdf = pd.HDFStore(filename)\n",
    "\n",
    "    # Store the dataframe in the HDFStore\n",
    "    hdf.put(path, df, format=\"table\")\n",
    "    print(f\"the data has been saved to {path}\")\n",
    "    # Close the HDFStore and HDF5 file\n",
    "    hdf.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping ImmoWelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imo\n",
    "url = \"https://www.immowelt.de/liste/muenchen/wohnungen/mieten?d=true&sd=DESC&sf=RELEVANCE&sp=\"\n",
    "select_statement = \"div.Pagination-190de span\"\n",
    "site_type = \"immowelt\"\n",
    "title = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page contains 30 subpages\n",
      "the following pages have been scrapped:\n",
      "https://www.immowelt.de/liste/muenchen/wohnungen/mieten?d=true&sd=DESC&sf=RELEVANCE&sp=1\n",
      "https://www.immowelt.de/liste/muenchen/wohnungen/mieten?d=true&sd=DESC&sf=RELEVANCE&sp=2\n",
      "https://www.immowelt.de/liste/muenchen/wohnungen/mieten?d=true&sd=DESC&sf=RELEVANCE&sp=3\n"
     ]
    }
   ],
   "source": [
    "scrape(url, select_statement, site_type, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving the fact section of the available renting objects in munich\n",
    "munich = combined_soup.find_all(\"div\", class_=\"FactsSection-52a7d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe with the core information\n",
    "col1 = []\n",
    "col2 = []\n",
    "col3 = []\n",
    "col4 = []\n",
    "\n",
    "for offer in munich:\n",
    "    col1.append(offer.text.split()[0].replace(\".\", \"\").split(\",\")[0])\n",
    "    col2.append(offer.text.split()[1].replace(\"€\", \"\"))\n",
    "    col3.append(offer.text.split()[2].replace(\"m²\", \"\"))\n",
    "    result = re.search(r\"!?location(.+?)(check|$)\", offer.text)\n",
    "    col4.append(result[1])\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"Rent\": col1, \"SQM\": col2, \"No. rooms\": col3, \"Area\": col4})\n",
    "df[\"Rent\"] = pd.to_numeric(df[\"Rent\"], errors=\"coerce\")\n",
    "df[\"SQM\"] = pd.to_numeric(df[\"SQM\"], errors=\"coerce\")\n",
    "df[\"No. rooms\"] = pd.to_numeric(df[\"No. rooms\"], errors=\"coerce\")\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping\n",
    "df_rooms = df.select_dtypes(include=[\"float\", \"int\"])\n",
    "df_rooms = df_rooms.groupby(\"No. rooms\").mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data has been saved to /immowelt/2023-05-23/df_immowelt_2023-05-23/\n",
      "entry for 2023-05-23 already exists. Data will be appended\n",
      "the data has been saved to /immowelt/2023-05-23/df_immowelt_rooms2023-05-23/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max_G\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tables\\path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'df_immowelt_2023-05-23'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "C:\\Users\\Max_G\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tables\\path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'df_immowelt_rooms2023-05-23'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    }
   ],
   "source": [
    "save_to_hdf5(filename, df, site_type, \"\")\n",
    "save_to_hdf5(filename, df_rooms, site_type, \"rooms\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping Booking.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booking\n",
    "url = \"https://www.booking.com/searchresults.html?label=gen173nr-1FCAEoggI46AdIM1gEaI4CiAEBmAExuAEYyAEP2AEB6AEB-AECiAIBqAIEuAKAteiiBsACAdICJDJmODU2ZGQwLTZjMzgtNGU4Yi05N2JiLTVmMTVmNTBiNWI5ZdgCBeACAQ&aid=304142&ss=Munich&efdco=1&lang=en-us&sb=1&src_elem=sb&src=index&dest_id=-1829149&dest_type=city&ac_position=0&ac_click_type=b&ac_langcode=en&ac_suggestion_list_length=5&search_selected=true&search_pageview_id=cc0f46c0a1a700b2&ac_meta=GhBjYzBmNDZjMGExYTcwMGIyIAAoATICZW46Bk11bmljaEAASgBQAA%3D%3D&checkin=2023-06-01&checkout=2023-06-07&group_adults=2&no_rooms=1&group_children=0&sb_travel_purpose=leisure&offset=1\"\n",
    "select_statement = \"li.f32a99c8d1\"\n",
    "site_type = \"booking\"\n",
    "title = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page contains 16 subpages\n",
      "the following pages have been scrapped:\n"
     ]
    }
   ],
   "source": [
    "scrape(url, select_statement, site_type, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_elements = combined_soup.find_all(\"div\", class_=\"d20f4628d0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe with the core information\n",
    "col1 = []\n",
    "col2 = []\n",
    "col3 = []\n",
    "col4 = []\n",
    "\n",
    "for div in div_elements:\n",
    "    col1.append(\n",
    "        div.find(\"div\", {\"data-testid\": \"title\", \"class\": \"fcab3ed991 a23c043802\"}).text\n",
    "    )\n",
    "    span = div.find(\"span\", {\"class\": \"c5888af24f e729ed5ab6\", \"aria-hidden\": \"true\"})\n",
    "    if span is not None:\n",
    "        col3.append(int(span.text.replace(\",\", \"\").replace(\"€\", \"\").replace(\" \", \"\")))\n",
    "        col2.append(\n",
    "            int(\n",
    "                div.find(\"span\", {\"data-testid\": \"price-and-discounted-price\"})\n",
    "                .text.replace(\",\", \"\")\n",
    "                .replace(\"€\", \"\")\n",
    "                .replace(\" \", \"\")\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        col2.append(-1)\n",
    "        col3.append(\n",
    "            int(\n",
    "                div.find(\"span\", {\"data-testid\": \"price-and-discounted-price\"})\n",
    "                .text.replace(\",\", \"\")\n",
    "                .replace(\"€\", \"\")\n",
    "                .replace(\" \", \"\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "df_hotel = pd.DataFrame({\"Name\": col1, \"Discounted\": col2, \"Undiscounted\": col3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry for 2023-05-22 already exists. Data will be appended\n",
      "the data has been saved to /booking/2023-05-22/df_booking_2023-05-22/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max_G\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tables\\path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'df_booking_2023-05-22'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    }
   ],
   "source": [
    "save_to_hdf5(filename, df_hotel, site_type, \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping Yahoo Finance \n",
    "for the five biggest German real estate companies that are stock companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five biggest according to\n",
    "# https://www.savills.de/research_articles/260049/291332-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating a dictionary with the websites and the necessary titles.\n",
    "# # Titles are need so that the functions knows its on the right site.\n",
    "# site_type = \"yahoo_finance\"\n",
    "# finance_url = {\n",
    "#     \"vonovia\": {\n",
    "#         \"url\": \"https://de.finance.yahoo.com/quote/VNA.DE?p=VNA.DE&.tsrc=fin-srch\",\n",
    "#         \"select_statement\": \"\",\n",
    "#         \"site_type\": \"yahoo\",\n",
    "#         \"title\": \"Vonovia SE (VNA.DE)\",\n",
    "#     },\n",
    "#     \"deutsche_wohnen\": {\n",
    "#         \"url\": \"https://de.finance.yahoo.com/quote/DWNI.DE?p=DWNI.DE&.tsrc=fin-srch\",\n",
    "#         \"select_statement\": \"\",\n",
    "#         \"site_type\": \"yahoo\",\n",
    "#         \"title\": \"Deutsche Wohnen SE (DWNI.DE)\",\n",
    "#     },\n",
    "#     \"saga\": {\n",
    "#         \"url\": \"https://de.finance.yahoo.com/quote/SAGA?p=SAGA&.tsrc=fin-srch\",\n",
    "#         \"select_statement\": \"\",\n",
    "#         \"site_type\": \"yahoo\",\n",
    "#         \"title\": \"Sagaliam Acquisition Corp. (SAGA)\",\n",
    "#     },\n",
    "#     \"leg\": {\n",
    "#         \"url\": \"https://de.finance.yahoo.com/quote/LEG.DE?p=LEG.DE&.tsrc=fin-srch\",\n",
    "#         \"select_statement\": \"\",\n",
    "#         \"site_type\": \"yahoo\",\n",
    "#         \"title\": \"LEG Immobilien SE (LEG.DE)\",\n",
    "#     },\n",
    "#     \"grand_city\": {\n",
    "#         \"url\": \"https://de.finance.yahoo.com/quote/GYC.DE?p=GYC.DE&.tsrc=fin-srch\",\n",
    "#         \"select_statement\": \"\",\n",
    "#         \"site_type\": \"yahoo\",\n",
    "#         \"title\": \"Grand City Properties S.A. (GYC.DE)\",\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col1 = []\n",
    "# col2 = []\n",
    "\n",
    "\n",
    "# # Scrapping the different stocks from the dictionary\n",
    "# for key, vonovia_data in finance_url.items():\n",
    "#     soup = scrape(\n",
    "#         vonovia_data[\"url\"],\n",
    "#         vonovia_data[\"select_statement\"],\n",
    "#         vonovia_data[\"site_type\"],\n",
    "#         vonovia_data[\"title\"],\n",
    "#     )\n",
    "#     div_elements_yahoo = soup.find(\"div\", class_=\"D(ib) Va(m) Maw(65%) Ov(h)\")\n",
    "\n",
    "#     col1.append(key)\n",
    "#     # in some instances there are multiple points in the price,\n",
    "#     # thus everything after the first point will be deleted\n",
    "#     parts = (\n",
    "#         re.sub(r\"[+\\-].*\", \"\", div_elements_yahoo.text.split()[0])\n",
    "#         .replace(\",\", \".\")\n",
    "#         .split(\".\")\n",
    "#     )\n",
    "#     string_stock_price = \".\".join(parts[:2])\n",
    "#     col2.append(float(string_stock_price))\n",
    "\n",
    "# df_stocks = pd.DataFrame({\"Name\": col1, \"Price\": col2})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping Boerse.de \n",
    "for the four biggest German real estate companies that are stock companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary with the websites and the necessary titles.\n",
    "# titles are needed so that the function knows it's on the right site.\n",
    "site_type = \"boerse\"\n",
    "\n",
    "finance_url = {\n",
    "    \"vonovia\": {\n",
    "        \"url\": \"https://www.boerse.de/aktien/Vonovia-Aktie/DE000A1ML7J1\",\n",
    "        \"select_statement\": \"\",\n",
    "        \"site_type\": \"boerse\",\n",
    "        \"title\": \"\",\n",
    "    },\n",
    "    \"deutsche_wohnen\": {\n",
    "        \"url\": \"https://www.boerse.de/aktien/Deutsche-Wohnen-Aktie/DE000A0HN5C6\",\n",
    "        \"select_statement\": \"\",\n",
    "        \"site_type\": \"boerse\",\n",
    "        \"title\": \"\",\n",
    "    },\n",
    "    \"leg\": {\n",
    "        \"url\": \"https://www.boerse.de/aktien/LEG-Immobilien-Aktie/DE000LEG1110\",\n",
    "        \"select_statement\": \"\",\n",
    "        \"site_type\": \"boerse\",\n",
    "        \"title\": \"\",\n",
    "    },\n",
    "    \"grand_city\": {\n",
    "        \"url\": \"https://www.boerse.de/aktien/Grand-City-Properties-Aktie/LU0775917882\",\n",
    "        \"select_statement\": \"\",\n",
    "        \"site_type\": \"boerse\",\n",
    "        \"title\": \"\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = []\n",
    "col2 = []\n",
    "\n",
    "\n",
    "# Scrapping the different stocks from the dictionary\n",
    "for key, data in finance_url.items():\n",
    "    soup = scrape(\n",
    "        data[\"url\"],\n",
    "        data[\"select_statement\"],\n",
    "        data[\"site_type\"],\n",
    "        data[\"title\"],\n",
    "    )\n",
    "\n",
    "    div_elements = soup.find_all(\"div\", class_=\"col-sm-12 col-xs-12\")\n",
    "\n",
    "    # Check if the 'div' element contains a 'span' element with the class 'BW_PUSH'\n",
    "    # this is necessary because the col-sm-12 col-xs-12 class is used more often, yet the combination is unique.\n",
    "    for div in div_elements:\n",
    "        if div.find(\"span\", class_=\"BW_PUSH\"):\n",
    "            share_price = div.find(\"span\", class_=\"BW_PUSH\").text\n",
    "            share_price = (\n",
    "                share_price.replace(\"\\n\", \"\").replace(\"EUR\", \"\").replace(\",\", \".\")\n",
    "            )\n",
    "\n",
    "    col1.append(key)\n",
    "    col2.append(float(share_price))\n",
    "\n",
    "df_stocks = pd.DataFrame({\"Name\": col1, \"Price\": col2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry for 2023-05-22 already exists. Data will be appended\n",
      "the data has been saved to /boerse/2023-05-22/df_boerse_2023-05-22/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max_G\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tables\\path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'df_boerse_2023-05-22'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    }
   ],
   "source": [
    "save_to_hdf5(filename, df_stocks, site_type, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = pd.HDFStore(filename, mode=\"r\")\n",
    "if len(hdf.keys()) == 4:\n",
    "    print(\"everything saved\")\n",
    "else:\n",
    "    print(\"Houston, we got a problem\")\n",
    "hdf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
